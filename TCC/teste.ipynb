{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","authorship_tag":"ABX9TyOaES2XpWxzwxmneHN7K45J"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount = True)\n","\n","driveDir = '/content/drive/Shareddrives/JCC/TCC/'\n","\n","!mkdir -p '/content/dataset/'\n","!unzip -qq -o '/content/drive/Shareddrives/JCC/TCC/dataset/real_fake' -d '/content/dataset'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QGGF4Ui5hcRU","executionInfo":{"status":"ok","timestamp":1700190357366,"user_tz":180,"elapsed":121563,"user":{"displayName":"João Pedro Rodrigues","userId":"08928887664543524629"}},"outputId":"a747b770-eaf0-4d00-ec32-8c25999aae9d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["#\n","# Dynamic Routing Between Capsules\n","# https://arxiv.org/pdf/1710.09829.pdf\n","#\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torchvision import datasets, transforms\n","import torch.nn.functional as F\n","\n","\n","class ConvUnit(nn.Module):\n","    def __init__(self, in_channels):\n","        super(ConvUnit, self).__init__()\n","\n","        self.conv0 = nn.Conv2d(in_channels=in_channels,\n","                               out_channels=32,  # fixme constant\n","                               kernel_size=9,  # fixme constant\n","                               stride=2, # fixme constant\n","                               bias=True)\n","\n","    def forward(self, x):\n","        return self.conv0(x)\n","\n","class CapsuleLayer(nn.Module):\n","    def __init__(self, in_units, in_channels, num_units, unit_size, use_routing):\n","        super(CapsuleLayer, self).__init__()\n","\n","        self.in_units = in_units\n","        self.in_channels = in_channels\n","        self.num_units = num_units\n","        self.use_routing = use_routing\n","\n","        if self.use_routing:\n","            # In the paper, the deeper capsule layer(s) with capsule inputs (DigitCaps) use a special routing algorithm\n","            # that uses this weight matrix.\n","            self.W = nn.Parameter(torch.randn(1, in_channels, num_units, unit_size, in_units))\n","        else:\n","            # The first convolutional capsule layer (PrimaryCapsules in the paper) does not perform routing.\n","            # Instead, it is composed of several convolutional units, each of which sees the full input.\n","            # It is implemented as a normal convolutional layer with a special nonlinearity (squash()).\n","            def create_conv_unit(unit_idx):\n","                unit = ConvUnit(in_channels=in_channels)\n","                self.add_module(\"unit_\" + str(unit_idx), unit)\n","                return unit\n","            self.units = [create_conv_unit(i) for i in range(self.num_units)]\n","\n","    @staticmethod\n","    def squash(s):\n","        # This is equation 1 from the paper.\n","        mag_sq = torch.sum(s**2, dim=2, keepdim=True)\n","        mag = torch.sqrt(mag_sq)\n","        s = (mag_sq / (1.0 + mag_sq)) * (s / mag)\n","        return s\n","\n","    def forward(self, x):\n","        if self.use_routing:\n","            return self.routing(x)\n","        else:\n","            return self.no_routing(x)\n","\n","    def no_routing(self, x):\n","        # Get output for each unit.\n","        # Each will be (batch, channels, height, width).\n","        u = [self.units[i](x) for i in range(self.num_units)]\n","\n","        # Stack all unit outputs (batch, unit, channels, height, width).\n","        u = torch.stack(u, dim=1)\n","\n","        # Flatten to (batch, unit, output).\n","        u = u.view(x.size(0), self.num_units, -1)\n","\n","        # Return squashed outputs.\n","        return CapsuleLayer.squash(u)\n","\n","    def routing(self, x):\n","        batch_size = x.size(0)\n","\n","        # (batch, in_units, features) -> (batch, features, in_units)\n","        x = x.transpose(1, 2)\n","\n","        # (batch, features, in_units) -> (batch, features, num_units, in_units, 1)\n","        x = torch.stack([x] * self.num_units, dim=2).unsqueeze(4)\n","\n","        # (batch, features, in_units, unit_size, num_units)\n","        W = torch.cat([self.W] * batch_size, dim=0)\n","\n","        # Transform inputs by weight matrix.\n","        # (batch_size, features, num_units, unit_size, 1)\n","        u_hat = torch.matmul(W, x)\n","\n","        # Initialize routing logits to zero.\n","        b_ij = Variable(torch.zeros(1, self.in_channels, self.num_units, 1)).cuda()\n","\n","        # Iterative routing.\n","        num_iterations = 3\n","        for iteration in range(num_iterations):\n","            # Convert routing logits to softmax.\n","            # (batch, features, num_units, 1, 1)\n","            c_ij = F.softmax(b_ij, dim=0)\n","            c_ij = torch.cat([c_ij] * batch_size, dim=0).unsqueeze(4)\n","\n","            # Apply routing (c_ij) to weighted inputs (u_hat).\n","            # (batch_size, 1, num_units, unit_size, 1)\n","            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True)\n","\n","            # (batch_size, 1, num_units, unit_size, 1)\n","            v_j = CapsuleLayer.squash(s_j)\n","\n","            # (batch_size, features, num_units, unit_size, 1)\n","            v_j1 = torch.cat([v_j] * self.in_channels, dim=1)\n","\n","            # (1, features, num_units, 1)\n","            u_vj1 = torch.matmul(u_hat.transpose(3, 4), v_j1).squeeze(4).mean(dim=0, keepdim=True)\n","\n","            # Update b_ij (routing)\n","            b_ij = b_ij + u_vj1\n","\n","        return v_j.squeeze(1)\n"],"metadata":{"id":"jJSG67Crhp4C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#\n","# Dynamic Routing Between Capsules\n","# https://arxiv.org/pdf/1710.09829.pdf\n","#\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torchvision import datasets, transforms\n","import torch.nn.functional as F\n","\n","\n","class CapsuleConvLayer(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(CapsuleConvLayer, self).__init__()\n","\n","        self.conv0 = nn.Conv2d(in_channels=in_channels,\n","                               out_channels=out_channels,\n","                               kernel_size=9, # fixme constant\n","                               stride=1,\n","                               bias=True)\n","\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, x):\n","        return self.relu(self.conv0(x))\n"],"metadata":{"id":"1ND-0V5MheyU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#\n","# Dynamic Routing Between Capsules\n","# https://arxiv.org/pdf/1710.09829.pdf\n","#\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torchvision import datasets, transforms\n","import torchvision.utils as vutils\n","import torch.nn.functional as F\n","\n","\n","class CapsuleNetwork(nn.Module):\n","    def __init__(self,\n","                 image_width,\n","                 image_height,\n","                 image_channels,\n","                 conv_inputs,\n","                 conv_outputs,\n","                 num_primary_units,\n","                 primary_unit_size,\n","                 num_output_units,\n","                 output_unit_size):\n","        super(CapsuleNetwork, self).__init__()\n","\n","        self.reconstructed_image_count = 0\n","\n","        self.image_channels = image_channels\n","        self.image_width = image_width\n","        self.image_height = image_height\n","\n","        self.conv1 = CapsuleConvLayer(in_channels=conv_inputs,\n","                                      out_channels=conv_outputs)\n","\n","        self.primary = CapsuleLayer(in_units=0,\n","                                    in_channels=conv_outputs,\n","                                    num_units=num_primary_units,\n","                                    unit_size=primary_unit_size,\n","                                    use_routing=False)\n","\n","        self.digits = CapsuleLayer(in_units=num_primary_units,\n","                                   in_channels=primary_unit_size,\n","                                   num_units=num_output_units,\n","                                   unit_size=output_unit_size,\n","                                   use_routing=True)\n","\n","        reconstruction_size = image_width * image_height * image_channels\n","        self.reconstruct0 = nn.Linear(num_output_units*output_unit_size, int((reconstruction_size * 2) / 3))\n","        self.reconstruct1 = nn.Linear(int((reconstruction_size * 2) / 3), int((reconstruction_size * 3) / 2))\n","        self.reconstruct2 = nn.Linear(int((reconstruction_size * 3) / 2), reconstruction_size)\n","\n","        self.relu = nn.ReLU(inplace=True)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        return self.digits(self.primary(self.conv1(x)))\n","\n","    def loss(self, images, input, target, size_average=True):\n","        return self.margin_loss(input, target, size_average) + self.reconstruction_loss(images, input, size_average)\n","\n","    def margin_loss(self, input, target, size_average=True):\n","        batch_size = input.size(0)\n","\n","        # ||vc|| from the paper.\n","        v_mag = torch.sqrt((input**2).sum(dim=2, keepdim=True))\n","\n","        # Calculate left and right max() terms from equation 4 in the paper.\n","        zero = Variable(torch.zeros(1)).cuda()\n","        m_plus = 0.9\n","        m_minus = 0.1\n","        max_l = torch.max(m_plus - v_mag, zero).view(batch_size, -1)**2\n","        max_r = torch.max(v_mag - m_minus, zero).view(batch_size, -1)**2\n","\n","        # This is equation 4 from the paper.\n","        loss_lambda = 0.5\n","        T_c = target\n","        L_c = T_c * max_l + loss_lambda * (1.0 - T_c) * max_r\n","        L_c = L_c.sum(dim=1)\n","\n","        if size_average:\n","            L_c = L_c.mean()\n","\n","        return L_c\n","\n","    def reconstruction_loss(self, images, input, size_average=True):\n","        # Get the lengths of capsule outputs.\n","        v_mag = torch.sqrt((input**2).sum(dim=2))\n","\n","        # Get index of longest capsule output.\n","        _, v_max_index = v_mag.max(dim=1)\n","        v_max_index = v_max_index.data\n","\n","        # Use just the winning capsule's representation (and zeros for other capsules) to reconstruct input image.\n","        batch_size = input.size(0)\n","        all_masked = [None] * batch_size\n","        for batch_idx in range(batch_size):\n","            # Get one sample from the batch.\n","            input_batch = input[batch_idx]\n","\n","            # Copy only the maximum capsule index from this batch sample.\n","            # This masks out (leaves as zero) the other capsules in this sample.\n","            batch_masked = Variable(torch.zeros(input_batch.size())).cuda()\n","            batch_masked[v_max_index[batch_idx]] = input_batch[v_max_index[batch_idx]]\n","            all_masked[batch_idx] = batch_masked\n","\n","        # Stack masked capsules over the batch dimension.\n","        masked = torch.stack(all_masked, dim=0)\n","\n","        # Reconstruct input image.\n","        masked = masked.view(input.size(0), -1)\n","        output = self.relu(self.reconstruct0(masked))\n","        output = self.relu(self.reconstruct1(output))\n","        output = self.sigmoid(self.reconstruct2(output))\n","        output = output.view(-1, self.image_channels, self.image_height, self.image_width)\n","\n","        # Save reconstructed images occasionally.\n","        if self.reconstructed_image_count % 10 == 0:\n","            if output.size(1) == 2:\n","                # handle two-channel images\n","                zeros = torch.zeros(output.size(0), 1, output.size(2), output.size(3))\n","                output_image = torch.cat([zeros, output.data.cpu()], dim=1)\n","            else:\n","                # assume RGB or grayscale\n","                output_image = output.data.cpu()\n","            vutils.save_image(output_image, \"reconstruction.png\")\n","        self.reconstructed_image_count += 1\n","\n","        # The reconstruction loss is the sum squared difference between the input image and reconstructed image.\n","        # Multiplied by a small number so it doesn't dominate the margin (class) loss.\n","        error = (output - images).view(output.size(0), -1)\n","        error = error**2\n","        error = torch.sum(error, dim=1) * 0.0005\n","\n","        # Average over batch\n","        if size_average:\n","            error = error.mean()\n","\n","        return error\n"],"metadata":{"id":"FTFt1gOVhfEp"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hdmk3nNddAcK","executionInfo":{"status":"ok","timestamp":1700191818690,"user_tz":180,"elapsed":1461355,"user":{"displayName":"João Pedro Rodrigues","userId":"08928887664543524629"}},"outputId":"4f1536ac-058c-4066-89e2-c3642bd1dab6"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 17500/17500 [24:10<00:00, 12.07it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Test Loss: 0.051503\tAccuracy: 69105/140000 (49%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import torch\n","import os\n","import csv\n","from tqdm import tqdm\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torchvision import datasets, transforms\n","import torch.nn.functional as F\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","batch_size = 8\n","image_size = 32\n","# Normalization for MNIST dataset.\n","dataset_transform = transforms.Compose([\n","    transforms.Resize((image_size, image_size)),\n","    transforms.ToTensor(),\n","    # transforms.Normalize((0.1307,), (0.3081,))\n","])\n","\n","test_dataset = datasets.ImageFolder(\n","    '/content/dataset/real_vs_fake/', transform=dataset_transform)\n","test_loader = torch.utils.data.DataLoader(\n","    test_dataset, batch_size=batch_size, shuffle = True)\n","\n","#\n","# Create capsule network.\n","#\n","\n","conv_inputs = 3\n","conv_outputs = 256\n","num_primary_units = 8\n","# primary_unit_size = 32 * 6 * 6  # fixme get from conv2d\n","primary_unit_size = 2048\n","output_unit_size = 16\n","\n","network = CapsuleNetwork(image_width=image_size,\n","                         image_height=image_size,\n","                         image_channels=3,\n","                         conv_inputs=conv_inputs,\n","                         conv_outputs=conv_outputs,\n","                         num_primary_units=num_primary_units,\n","                         primary_unit_size=primary_unit_size,\n","                         num_output_units=2,\n","                         output_unit_size=output_unit_size).cuda()\n","\n","# Converts batches of class indices to classes of one-hot vectors.\n","def to_one_hot(x, length):\n","    batch_size = x.size(0)\n","    x_one_hot = torch.zeros(batch_size, length)\n","    for i in range(batch_size):\n","        x_one_hot[i, x[i]] = 1.0\n","    return x_one_hot\n","\n","\n","checkpoint = torch.load(driveDir + 'Modelos/model_24.pth')\n","network.load_state_dict(checkpoint['model'])\n","\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","network.to(DEVICE)\n","\n","y_pred = []\n","y_real = []\n","\n","def test():\n","    network.eval()\n","    test_loss = 0\n","    correct = 0\n","    for data, target in tqdm(test_loader):\n","        target_indices = target\n","        target_one_hot = to_one_hot(\n","            target_indices, length=network.digits.num_units)\n","\n","        data = data.cuda()\n","        target = target_one_hot.cuda()\n","\n","        with torch.set_grad_enabled(False):\n","            output = network(data)\n","\n","            # sum up batch loss\n","            test_loss += network.loss(data, output, target).data\n","\n","        v_mag = torch.sqrt((output**2).sum(dim=2, keepdim=True))\n","\n","        pred = v_mag.data.max(1, keepdim=True)[1].cpu()\n","\n","        y_pred.append(pred)\n","\n","        temp = correct\n","        correct += pred.eq(target_indices.view_as(pred)).sum()\n","\n","        if temp != correct:\n","          y_real.append(1-pred)\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest Loss: {:.6f}\\tAccuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss,\n","        correct,\n","        len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","\n","    return test_loss, 100. * correct / len(test_loader.dataset)\n","\n","_, acc = test()\n"]}]}
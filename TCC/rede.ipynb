{"cells":[{"cell_type":"code","source":["# function ClickConnect(){\n","# console.log(\"Working\");\n","# document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click();\n","# }\n","# interval = setInterval(ClickConnect,60000)"],"metadata":{"id":"ug4US5TXjpAN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount = True)\n","\n","driveDir = '/content/drive/Shareddrives/JCC/TCC/'\n","\n","!mkdir -p '/content/dataset/'\n","!unzip -qq -o '/content/drive/Shareddrives/JCC/TCC/dataset/real_fake' -d '/content/dataset'\n","\n","!mkdir -p '/content/drive/Shareddrives/JCC/TCC/Resultados/'\n","!mkdir -p '/content/drive/Shareddrives/JCC/TCC/Modelos/'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9cMYm99EZ5y-","executionInfo":{"status":"ok","timestamp":1695567434206,"user_tz":180,"elapsed":68047,"user":{"displayName":"JoÃ£o Pedro Rodrigues","userId":"08928887664543524629"}},"outputId":"251c22ba-2ba8-4590-94cb-97e697728d66"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"76ke5Jm1Yor1"},"outputs":[],"source":["#\n","# Dynamic Routing Between Capsules\n","# https://arxiv.org/pdf/1710.09829.pdf\n","#\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torchvision import datasets, transforms\n","import torch.nn.functional as F\n","\n","\n","class ConvUnit(nn.Module):\n","    def __init__(self, in_channels):\n","        super(ConvUnit, self).__init__()\n","\n","        self.conv0 = nn.Conv2d(in_channels=in_channels,\n","                               out_channels=32,  # fixme constant\n","                               kernel_size=9,  # fixme constant\n","                               stride=2, # fixme constant\n","                               bias=True)\n","\n","    def forward(self, x):\n","        return self.conv0(x)\n","\n","class CapsuleLayer(nn.Module):\n","    def __init__(self, in_units, in_channels, num_units, unit_size, use_routing):\n","        super(CapsuleLayer, self).__init__()\n","\n","        self.in_units = in_units\n","        self.in_channels = in_channels\n","        self.num_units = num_units\n","        self.use_routing = use_routing\n","\n","        if self.use_routing:\n","            # In the paper, the deeper capsule layer(s) with capsule inputs (DigitCaps) use a special routing algorithm\n","            # that uses this weight matrix.\n","            self.W = nn.Parameter(torch.randn(1, in_channels, num_units, unit_size, in_units))\n","        else:\n","            # The first convolutional capsule layer (PrimaryCapsules in the paper) does not perform routing.\n","            # Instead, it is composed of several convolutional units, each of which sees the full input.\n","            # It is implemented as a normal convolutional layer with a special nonlinearity (squash()).\n","            def create_conv_unit(unit_idx):\n","                unit = ConvUnit(in_channels=in_channels)\n","                self.add_module(\"unit_\" + str(unit_idx), unit)\n","                return unit\n","            self.units = [create_conv_unit(i) for i in range(self.num_units)]\n","\n","    @staticmethod\n","    def squash(s):\n","        # This is equation 1 from the paper.\n","        mag_sq = torch.sum(s**2, dim=2, keepdim=True)\n","        mag = torch.sqrt(mag_sq)\n","        s = (mag_sq / (1.0 + mag_sq)) * (s / mag)\n","        return s\n","\n","    def forward(self, x):\n","        if self.use_routing:\n","            return self.routing(x)\n","        else:\n","            return self.no_routing(x)\n","\n","    def no_routing(self, x):\n","        # Get output for each unit.\n","        # Each will be (batch, channels, height, width).\n","        u = [self.units[i](x) for i in range(self.num_units)]\n","\n","        # Stack all unit outputs (batch, unit, channels, height, width).\n","        u = torch.stack(u, dim=1)\n","\n","        # Flatten to (batch, unit, output).\n","        u = u.view(x.size(0), self.num_units, -1)\n","\n","        # Return squashed outputs.\n","        return CapsuleLayer.squash(u)\n","\n","    def routing(self, x):\n","        batch_size = x.size(0)\n","\n","        # (batch, in_units, features) -> (batch, features, in_units)\n","        x = x.transpose(1, 2)\n","\n","        # (batch, features, in_units) -> (batch, features, num_units, in_units, 1)\n","        x = torch.stack([x] * self.num_units, dim=2).unsqueeze(4)\n","\n","        # (batch, features, in_units, unit_size, num_units)\n","        W = torch.cat([self.W] * batch_size, dim=0)\n","\n","        # Transform inputs by weight matrix.\n","        # (batch_size, features, num_units, unit_size, 1)\n","        u_hat = torch.matmul(W, x)\n","\n","        # Initialize routing logits to zero.\n","        b_ij = Variable(torch.zeros(1, self.in_channels, self.num_units, 1)).cuda()\n","\n","        # Iterative routing.\n","        num_iterations = 3\n","        for iteration in range(num_iterations):\n","            # Convert routing logits to softmax.\n","            # (batch, features, num_units, 1, 1)\n","            c_ij = F.softmax(b_ij, dim=0)\n","            c_ij = torch.cat([c_ij] * batch_size, dim=0).unsqueeze(4)\n","\n","            # Apply routing (c_ij) to weighted inputs (u_hat).\n","            # (batch_size, 1, num_units, unit_size, 1)\n","            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True)\n","\n","            # (batch_size, 1, num_units, unit_size, 1)\n","            v_j = CapsuleLayer.squash(s_j)\n","\n","            # (batch_size, features, num_units, unit_size, 1)\n","            v_j1 = torch.cat([v_j] * self.in_channels, dim=1)\n","\n","            # (1, features, num_units, 1)\n","            u_vj1 = torch.matmul(u_hat.transpose(3, 4), v_j1).squeeze(4).mean(dim=0, keepdim=True)\n","\n","            # Update b_ij (routing)\n","            b_ij = b_ij + u_vj1\n","\n","        return v_j.squeeze(1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZfDerPRRYor6"},"outputs":[],"source":["#\n","# Dynamic Routing Between Capsules\n","# https://arxiv.org/pdf/1710.09829.pdf\n","#\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torchvision import datasets, transforms\n","import torch.nn.functional as F\n","\n","\n","class CapsuleConvLayer(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(CapsuleConvLayer, self).__init__()\n","\n","        self.conv0 = nn.Conv2d(in_channels=in_channels,\n","                               out_channels=out_channels,\n","                               kernel_size=9, # fixme constant\n","                               stride=1,\n","                               bias=True)\n","\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, x):\n","        return self.relu(self.conv0(x))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VV5rBCwWYor7"},"outputs":[],"source":["#\n","# Dynamic Routing Between Capsules\n","# https://arxiv.org/pdf/1710.09829.pdf\n","#\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torchvision import datasets, transforms\n","import torchvision.utils as vutils\n","import torch.nn.functional as F\n","\n","\n","class CapsuleNetwork(nn.Module):\n","    def __init__(self,\n","                 image_width,\n","                 image_height,\n","                 image_channels,\n","                 conv_inputs,\n","                 conv_outputs,\n","                 num_primary_units,\n","                 primary_unit_size,\n","                 num_output_units,\n","                 output_unit_size):\n","        super(CapsuleNetwork, self).__init__()\n","\n","        self.reconstructed_image_count = 0\n","\n","        self.image_channels = image_channels\n","        self.image_width = image_width\n","        self.image_height = image_height\n","\n","        self.conv1 = CapsuleConvLayer(in_channels=conv_inputs,\n","                                      out_channels=conv_outputs)\n","\n","        self.primary = CapsuleLayer(in_units=0,\n","                                    in_channels=conv_outputs,\n","                                    num_units=num_primary_units,\n","                                    unit_size=primary_unit_size,\n","                                    use_routing=False)\n","\n","        self.digits = CapsuleLayer(in_units=num_primary_units,\n","                                   in_channels=primary_unit_size,\n","                                   num_units=num_output_units,\n","                                   unit_size=output_unit_size,\n","                                   use_routing=True)\n","\n","        reconstruction_size = image_width * image_height * image_channels\n","        self.reconstruct0 = nn.Linear(num_output_units*output_unit_size, int((reconstruction_size * 2) / 3))\n","        self.reconstruct1 = nn.Linear(int((reconstruction_size * 2) / 3), int((reconstruction_size * 3) / 2))\n","        self.reconstruct2 = nn.Linear(int((reconstruction_size * 3) / 2), reconstruction_size)\n","\n","        self.relu = nn.ReLU(inplace=True)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        return self.digits(self.primary(self.conv1(x)))\n","\n","    def loss(self, images, input, target, size_average=True):\n","        return self.margin_loss(input, target, size_average) + self.reconstruction_loss(images, input, size_average)\n","\n","    def margin_loss(self, input, target, size_average=True):\n","        batch_size = input.size(0)\n","\n","        # ||vc|| from the paper.\n","        v_mag = torch.sqrt((input**2).sum(dim=2, keepdim=True))\n","\n","        # Calculate left and right max() terms from equation 4 in the paper.\n","        zero = Variable(torch.zeros(1)).cuda()\n","        m_plus = 0.9\n","        m_minus = 0.1\n","        max_l = torch.max(m_plus - v_mag, zero).view(batch_size, -1)**2\n","        max_r = torch.max(v_mag - m_minus, zero).view(batch_size, -1)**2\n","\n","        # This is equation 4 from the paper.\n","        loss_lambda = 0.5\n","        T_c = target\n","        L_c = T_c * max_l + loss_lambda * (1.0 - T_c) * max_r\n","        L_c = L_c.sum(dim=1)\n","\n","        if size_average:\n","            L_c = L_c.mean()\n","\n","        return L_c\n","\n","    def reconstruction_loss(self, images, input, size_average=True):\n","        # Get the lengths of capsule outputs.\n","        v_mag = torch.sqrt((input**2).sum(dim=2))\n","\n","        # Get index of longest capsule output.\n","        _, v_max_index = v_mag.max(dim=1)\n","        v_max_index = v_max_index.data\n","\n","        # Use just the winning capsule's representation (and zeros for other capsules) to reconstruct input image.\n","        batch_size = input.size(0)\n","        all_masked = [None] * batch_size\n","        for batch_idx in range(batch_size):\n","            # Get one sample from the batch.\n","            input_batch = input[batch_idx]\n","\n","            # Copy only the maximum capsule index from this batch sample.\n","            # This masks out (leaves as zero) the other capsules in this sample.\n","            batch_masked = Variable(torch.zeros(input_batch.size())).cuda()\n","            batch_masked[v_max_index[batch_idx]] = input_batch[v_max_index[batch_idx]]\n","            all_masked[batch_idx] = batch_masked\n","\n","        # Stack masked capsules over the batch dimension.\n","        masked = torch.stack(all_masked, dim=0)\n","\n","        # Reconstruct input image.\n","        masked = masked.view(input.size(0), -1)\n","        output = self.relu(self.reconstruct0(masked))\n","        output = self.relu(self.reconstruct1(output))\n","        output = self.sigmoid(self.reconstruct2(output))\n","        output = output.view(-1, self.image_channels, self.image_height, self.image_width)\n","\n","        # Save reconstructed images occasionally.\n","        if self.reconstructed_image_count % 10 == 0:\n","            if output.size(1) == 2:\n","                # handle two-channel images\n","                zeros = torch.zeros(output.size(0), 1, output.size(2), output.size(3))\n","                output_image = torch.cat([zeros, output.data.cpu()], dim=1)\n","            else:\n","                # assume RGB or grayscale\n","                output_image = output.data.cpu()\n","            vutils.save_image(output_image, \"reconstruction.png\")\n","        self.reconstructed_image_count += 1\n","\n","        # The reconstruction loss is the sum squared difference between the input image and reconstructed image.\n","        # Multiplied by a small number so it doesn't dominate the margin (class) loss.\n","        error = (output - images).view(output.size(0), -1)\n","        error = error**2\n","        error = torch.sum(error, dim=1) * 0.0005\n","\n","        # Average over batch\n","        if size_average:\n","            error = error.mean()\n","\n","        return error\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vlh9FU7jYor8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695604002500,"user_tz":180,"elapsed":36564150,"user":{"displayName":"JoÃ£o Pedro Rodrigues","userId":"08928887664543524629"}},"outputId":"cb46b724-bec4-490a-97b3-eccde1f3c178"},"outputs":[{"output_type":"stream","name":"stdout","text":["1 - Epoch: \n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 12500/12500 [22:22<00:00,  9.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.034893\n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 2500/2500 [02:48<00:00, 14.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Loss: 0.025715\tAccuracy: 14633/20000 (73%)\n","\n","2 - Epoch: \n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 12500/12500 [22:05<00:00,  9.43it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.023691\n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 2500/2500 [02:44<00:00, 15.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Loss: 0.021564\tAccuracy: 15879/20000 (79%)\n","\n","3 - Epoch: \n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 12500/12500 [22:03<00:00,  9.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.020297\n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 2500/2500 [02:44<00:00, 15.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Loss: 0.019648\tAccuracy: 16402/20000 (82%)\n","\n","4 - Epoch: \n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 12500/12500 [22:01<00:00,  9.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.018219\n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 2500/2500 [02:50<00:00, 14.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Loss: 0.018981\tAccuracy: 16609/20000 (83%)\n","\n","5 - Epoch: \n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 12500/12500 [22:02<00:00,  9.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.016696\n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 2500/2500 [02:45<00:00, 15.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Loss: 0.016451\tAccuracy: 17206/20000 (86%)\n","\n","6 - Epoch: \n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 12500/12500 [21:57<00:00,  9.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.015584\n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 2500/2500 [02:45<00:00, 15.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Loss: 0.015408\tAccuracy: 17491/20000 (87%)\n","\n","7 - Epoch: \n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 12500/12500 [21:43<00:00,  9.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.014517\n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 2500/2500 [02:49<00:00, 14.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Loss: 0.015100\tAccuracy: 17598/20000 (88%)\n","\n","8 - Epoch: \n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 12500/12500 [21:47<00:00,  9.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.013752\n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 2500/2500 [02:47<00:00, 14.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Loss: 0.017445\tAccuracy: 16762/20000 (84%)\n","\n","9 - Epoch: \n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 12500/12500 [21:32<00:00,  9.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.013042\n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 2500/2500 [02:44<00:00, 15.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Loss: 0.013890\tAccuracy: 17904/20000 (90%)\n","\n","10 - Epoch: \n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 12500/12500 [21:27<00:00,  9.71it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.012464\n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 2500/2500 [02:46<00:00, 15.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Loss: 0.013922\tAccuracy: 17836/20000 (89%)\n","\n","11 - Epoch: \n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 12500/12500 [21:23<00:00,  9.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.011941\n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 2500/2500 [02:45<00:00, 15.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Loss: 0.013641\tAccuracy: 17944/20000 (90%)\n","\n","12 - Epoch: \n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 12500/12500 [21:30<00:00,  9.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.011408\n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 2500/2500 [02:49<00:00, 14.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Loss: 0.012792\tAccuracy: 18128/20000 (91%)\n","\n","13 - Epoch: \n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 12500/12500 [21:25<00:00,  9.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.011022\n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 2500/2500 [02:45<00:00, 15.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Loss: 0.012874\tAccuracy: 18145/20000 (91%)\n","\n","14 - Epoch: \n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 12500/12500 [21:29<00:00,  9.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.010690\n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 2500/2500 [02:46<00:00, 14.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Loss: 0.012464\tAccuracy: 18197/20000 (91%)\n","\n","15 - Epoch: \n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 12500/12500 [21:24<00:00,  9.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.010273\n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 2500/2500 [02:47<00:00, 14.90it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Loss: 0.012800\tAccuracy: 18184/20000 (91%)\n","\n","16 - Epoch: \n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 12500/12500 [21:23<00:00,  9.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.009985\n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 2500/2500 [02:47<00:00, 14.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Loss: 0.012960\tAccuracy: 18111/20000 (91%)\n","\n","17 - Epoch: \n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 12500/12500 [21:29<00:00,  9.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.009702\n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 2500/2500 [02:48<00:00, 14.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Loss: 0.012649\tAccuracy: 18229/20000 (91%)\n","\n","18 - Epoch: \n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 12500/12500 [21:22<00:00,  9.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.009423\n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 2500/2500 [02:47<00:00, 14.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Loss: 0.012004\tAccuracy: 18203/20000 (91%)\n","\n","19 - Epoch: \n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 12500/12500 [21:25<00:00,  9.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.009228\n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 2500/2500 [02:41<00:00, 15.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Loss: 0.011741\tAccuracy: 18341/20000 (92%)\n","\n","20 - Epoch: \n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 12500/12500 [21:20<00:00,  9.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.009009\n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 2500/2500 [02:44<00:00, 15.16it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Loss: 0.011744\tAccuracy: 18320/20000 (92%)\n","\n","21 - Epoch: \n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 12500/12500 [21:19<00:00,  9.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.008746\n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 2500/2500 [02:43<00:00, 15.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Loss: 0.012046\tAccuracy: 18246/20000 (91%)\n","\n","22 - Epoch: \n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 12500/12500 [21:17<00:00,  9.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.008557\n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 2500/2500 [02:43<00:00, 15.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Loss: 0.011718\tAccuracy: 18333/20000 (92%)\n","\n","23 - Epoch: \n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 12500/12500 [21:18<00:00,  9.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.008330\n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 2500/2500 [02:43<00:00, 15.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Loss: 0.011651\tAccuracy: 18289/20000 (91%)\n","\n","24 - Epoch: \n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 12500/12500 [21:21<00:00,  9.75it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.008194\n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 2500/2500 [02:44<00:00, 15.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Loss: 0.011427\tAccuracy: 18415/20000 (92%)\n","\n","25 - Epoch: \n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 12500/12500 [21:14<00:00,  9.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.008015\n"]},{"output_type":"stream","name":"stderr","text":["100%|ââââââââââ| 2500/2500 [02:45<00:00, 15.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test Loss: 0.011699\tAccuracy: 18329/20000 (92%)\n","\n"]}],"source":["#\n","# Dynamic Routing Between Capsules\n","# https://arxiv.org/pdf/1710.09829.pdf\n","#\n","\n","import torch\n","import os\n","import csv\n","from tqdm import tqdm\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torchvision import datasets, transforms\n","import torch.nn.functional as F\n","\n","#\n","# Settings.\n","#\n","\n","learning_rate = 0.001\n","\n","batch_size = 8\n","\n","# Stop training if loss goes below this threshold.\n","early_stop_loss = 0.0001\n","\n","image_size = 32\n","# Normalization for MNIST dataset.\n","dataset_transform = transforms.Compose([\n","    transforms.Resize(image_size),\n","    transforms.ToTensor(),\n","    # transforms.Normalize((0.1307,), (0.3081,))\n","])\n","\n","train_dataset = datasets.ImageFolder(\n","    '/content/dataset/real_vs_fake/real-vs-fake/train/', transform=dataset_transform)\n","train_loader = torch.utils.data.DataLoader(\n","    train_dataset, batch_size=batch_size, shuffle=True)\n","\n","val_dataset = datasets.ImageFolder(\n","    '/content/dataset/real_vs_fake/real-vs-fake/valid/', transform=dataset_transform)\n","val_loader = torch.utils.data.DataLoader(\n","    val_dataset, batch_size=batch_size, shuffle=True)\n","\n","#\n","# Create capsule network.\n","#\n","\n","conv_inputs = 3\n","conv_outputs = 256\n","num_primary_units = 8\n","# primary_unit_size = 32 * 6 * 6  # fixme get from conv2d\n","primary_unit_size = 2048\n","output_unit_size = 16\n","\n","network = CapsuleNetwork(image_width=image_size,\n","                         image_height=image_size,\n","                         image_channels=3,\n","                         conv_inputs=conv_inputs,\n","                         conv_outputs=conv_outputs,\n","                         num_primary_units=num_primary_units,\n","                         primary_unit_size=primary_unit_size,\n","                         num_output_units=2,  # one for each MNIST digit\n","                         output_unit_size=output_unit_size).cuda()\n","# print(network)\n","\n","\n","# Converts batches of class indices to classes of one-hot vectors.\n","def to_one_hot(x, length):\n","    batch_size = x.size(0)\n","    x_one_hot = torch.zeros(batch_size, length)\n","    for i in range(batch_size):\n","        x_one_hot[i, x[i]] = 1.0\n","    return x_one_hot\n","\n","# This is the test function from the basic Pytorch MNIST example, but adapted to use the capsule network.\n","# https://github.com/pytorch/examples/blob/master/mnist/main.py\n","\n","\n","def test():\n","    network.eval()\n","    test_loss = 0\n","    correct = 0\n","    for data, target in tqdm(val_loader):\n","        target_indices = target\n","        target_one_hot = to_one_hot(\n","            target_indices, length=network.digits.num_units)\n","\n","        data = data.cuda()\n","        target = target_one_hot.cuda()\n","\n","        with torch.set_grad_enabled(False):\n","            output = network(data)\n","\n","            # sum up batch loss\n","            test_loss += network.loss(data, output, target).data\n","\n","        v_mag = torch.sqrt((output**2).sum(dim=2, keepdim=True))\n","\n","        pred = v_mag.data.max(1, keepdim=True)[1].cpu()\n","\n","        correct += pred.eq(target_indices.view_as(pred)).sum()\n","\n","    test_loss /= len(val_loader.dataset)\n","\n","    print('\\nTest Loss: {:.6f}\\tAccuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss,\n","        correct,\n","        len(val_loader.dataset),\n","        100. * correct / len(val_loader.dataset)))\n","\n","    return test_loss, 100. * correct / len(val_loader.dataset)\n","\n","\n","# This is the train function from the basic Pytorch MNIST example, but adapted to use the capsule network.\n","# https://github.com/pytorch/examples/blob/master/mnist/main.py\n","def train(optimizer):\n","    network.train()\n","    train_loss = 0.0\n","    for (data, target) in tqdm(train_loader):\n","        target_one_hot = to_one_hot(target, length=network.digits.num_units)\n","\n","        data, target = data.cuda(), target_one_hot.cuda()\n","\n","        optimizer.zero_grad()\n","\n","        with torch.set_grad_enabled(True):\n","            output = network(data)\n","\n","            loss = network.loss(data, output, target)\n","            loss.backward()\n","\n","            optimizer.step()\n","\n","            train_loss += loss.data\n","\n","        # if loss.data < early_stop_loss:\n","        #     break\n","\n","    train_loss /= len(train_loader.dataset)\n","\n","    print('Train Loss: {:.6f}'.format(\n","                train_loss))\n","\n","    return train_loss\n","\n","\n","num_epochs = 25\n","list_train = []\n","list_test = []\n","list_accuracy = []\n","optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n","\n","for epoch in range(1, num_epochs + 1):\n","    print('{} - Epoch: '.format(epoch))\n","    list_train.append(train(optimizer).item())\n","    test_loss, accuracy = test()\n","    list_test.append(test_loss.item())\n","    list_accuracy.append(accuracy.item())\n","\n","    with open(f'{driveDir}/Resultados/losses_{epoch}', 'w') as csvFile:\n","        writer = csv.writer(csvFile)\n","\n","        for i in range(len(list_train)):\n","            writer.writerow([list_train[i], list_test[i], list_accuracy[i]])\n","\n","        csvFile.close()\n","\n","    torch.save({\n","        'model': network.state_dict(),\n","        'optimizer': optimizer.state_dict(),\n","    }, f'{driveDir}/Modelos/model_{epoch}.pth')\n","\n","    if list_train[-1] < early_stop_loss:\n","        break"]}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4,"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}